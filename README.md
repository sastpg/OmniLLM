# OmniLLM

<div align="center">
    <a href="https://github.com/sastpg/OmniLLM/tree/main">
        <img src="https://img.shields.io/badge/python-3.12-blue" alt="Python Version"/>
    </a>
    <a href="https://github.com/sastpg/OmniLLM/blob/main/LICENSE">
        <img src="https://img.shields.io/badge/license-MIT-green" alt="MIT"/>
    </a>
</div>

âš¡ Build open-source large language model(LLM) service âš¡

Read this in [English](README_en.md)

![](./images/web_demo.png)

## é¡¹ç›®æ›´æ–°

- ``2024/07/14``: æ”¯æŒæ¨¡å‹æµå¼è¾“å‡ºã€‚
- ğŸ”¥ ``2024/07/08``: å®ç°å·¥å…·è°ƒç”¨ï¼ˆä¹Ÿå«å‡½æ•°è°ƒç”¨ï¼‰çš„ API æ”¯æŒï¼Œæ›´å¤šè°ƒç”¨ç»†èŠ‚è¯·å‰å¾€ [æŸ¥çœ‹]()ã€‚
- ğŸ”¥ ``2024/06/17``: æ”¯æŒ LLaMA3, Qwen2, GLM4 ä»¥åŠå®ƒä»¬å¾®è°ƒåæ¨¡å‹çš„ API è°ƒç”¨ï¼Œå‘å¸ƒåœ¨ç½‘ç«™ä¸­ä½¿ç”¨çš„ç®€å•ç¤ºä¾‹ã€‚

## å¿«é€Ÿå¼€å§‹
### Python ç¯å¢ƒ
æ¨èæ‚¨ä½¿ç”¨ Conda è¿›è¡Œ python åŒ…ç®¡ç†ï¼Œå…³äºå¦‚ä½•å®‰è£…å’Œä½¿ç”¨ Conda è¯·è§[Conda å®˜æ–¹æ–‡æ¡£](https://conda.io/en/latest/index.html)ã€‚

1. å…‹éš†æœ¬ä»“åº“å¹¶è¿›å…¥é¡¹ç›®æ–‡ä»¶å¤¹
```bash
git clone https://github.com/sastpg/OmniLLM.git
cd OmniLLM
```

2. å®‰è£… Python åŒ…
```bash
conda create -n omnillm python=3.12
conda activate omnillm
cd service  # æ¨¡å‹ API æœåŠ¡
pip install -r requirements.txt
cd ../web  # ä»¥ä¸‹å‘½ä»¤å¯ä¸æ‰§è¡Œï¼Œå¦‚æœæ‚¨åªéœ€è¦ API æœåŠ¡è€Œä¸éœ€è¦ç½‘é¡µ Demo
pip install -r requirements.txt
```

### æ¨¡å‹ä¸‹è½½
1. åœ¨é­”å¡”ï¼ˆå¼ºçƒˆå»ºè®®ä¸­å›½å¤§é™†çš„ç”¨æˆ·ä½¿ç”¨é­”å¡”ï¼‰æˆ–è€… Huggingface ä¸Šä¸‹è½½ LLaMA3-8Bï¼ŒGLM4-9Bï¼ŒQwen2-7B ç­‰æ¨¡å‹ï¼Œæœ¬æ­¥éª¤è€—æ—¶è¾ƒä¹…ï¼Œå¯ä»¥é€‰æ‹©éœ€è¦çš„æ¨¡å‹ä¸‹è½½ï¼š
```bash
cd ..  # è¿›å…¥é¡¹ç›®æ–‡ä»¶å¤¹æ ¹ç›®å½•
git clone https://www.modelscope.cn/LLM-Research/Meta-Llama-3-8B-Instruct.git
git clone https://www.modelscope.cn/qwen/Qwen2-7B-Instruct.git
git clone https://www.modelscope.cn/ZhipuAI/glm-4-9b-chat.git
```
2. è¿›å…¥ `config/config.json` æ–‡ä»¶è¿›è¡Œé…ç½®ï¼š
```json
{
    "CUDA_VISIBLE_DEVICES": "0, 1, 2",  // æ˜¾å¡èµ„æº
    "models":{
        "llama-3-8b-instruct": "../models/Meta-Llama-3-8B-Instruct",  // æ¨¡å‹æ–‡ä»¶çš„è·¯å¾„
        "glm-4-9b-chat": "../models/glm-4-9b-chat",
        "qwen-2-7b-instruct": "../models/Qwen2-7B-Instruct"
    },
    "port": 8001  // API æœåŠ¡çš„ç«¯å£å·
}
```

### ğŸ¤– æ¨¡å‹ API æœåŠ¡
```bash
cd service
python main.py
```

å¯åŠ¨æ¨¡å‹ API æœåŠ¡åï¼Œè®¿é—® http://localhost:8001/docs å¦‚ä¸‹æ‰€ç¤ºï¼ˆå¦‚åœ¨æœåŠ¡å™¨ä¸Šéƒ¨ç½²ï¼Œå°† localhost æ›¿æ¢ä¸ºæœåŠ¡å™¨å…¬ç½‘ IPï¼‰ï¼š

![](./images/api.png)

### ğŸ–¥ï¸ ç½‘é¡µ Demo
ä¸€ä¸ªç®€å•çš„ç”¨æˆ·äº¤äº’ç•Œé¢ï¼Œè°ƒç”¨ æ¨¡å‹ API æœåŠ¡ç”Ÿæˆå›ç­”ï¼Œå¹¶å±•ç¤ºåœ¨ç½‘é¡µä¸Šã€‚
```bash
cd ../web
streamlit run chat.py --server.port 8601
```

å‘½ä»¤è¿è¡Œåï¼Œç‚¹å‡»ä¸‹æ–¹çš„é“¾æ¥å³å¯è®¿é—®ç½‘é¡µã€‚

## API è°ƒç”¨
æ‚¨å¯ä»¥é€šè¿‡ HTTP è°ƒç”¨æ¥å£æ¥ä½¿ç”¨æ¨¡å‹ï¼Œå®ç°å•è½®å¯¹è¯ã€å¤šè½®å¯¹è¯ã€æµå¼è¾“å‡ºã€å·¥å…·è°ƒç”¨ç­‰å¤šç§åŠŸèƒ½ã€‚æœ¬é¡¹ç›®ä¹Ÿæä¾›äº† Python æ¥å£è°ƒç”¨çš„æ–¹å¼æ¥ä½¿ç”¨ï¼Œä»£ç å¦‚ä¸‹ï¼š

```python
def call_llm(model_name, messages, tools=None, **kwargs):
    url = f"http://localhost:8001/{model_name}"
    message = {
        "messages": messages,
        "tools": tools,
        "llm_config": kwargs
    }
    print(model_name, message)
    resp = requests.post(url, json=message)
    return resp.json()
```
### å‚æ•°è¯´æ˜
ğŸ’¡ ä»¥ä¸‹è¯´æ˜ä»…é’ˆå¯¹ Python æ¥å£ï¼Œæ›´å¤šç»†èŠ‚è¯·è§è¯¦ç»†æ–‡æ¡£ã€‚

| å‚æ•°                   | æ•°æ®ç±»å‹   | é»˜è®¤å€¼ | è¯´æ˜                                                         |
| ---------------------- | ---------- | ------ | ------------------------------------------------------------ |
| model_nameï¼ˆå¿…é€‰ï¼‰     | string     | æ—      | æŒ‡å®šç”¨äºå¯¹è¯çš„é€šä¹‰åƒé—®æ¨¡å‹åï¼Œç›®å‰å¯é€‰æ‹© `llama-3-8b-instruct`ã€`llama-3-8b-finetune`ã€`glm-4-9b-chat`ã€`qwen-2-7b-instruct`ã€‚ |
| messagesï¼ˆå¿…é€‰ï¼‰       | list[dict] | æ—      | ç”¨æˆ·ä¸æ¨¡å‹çš„å¯¹è¯å†å²ã€‚list ä¸­æ¯ä¸ªå…ƒç´ å½¢å¼ä¸º`{"role":è§’è‰², "content": å†…å®¹}`ï¼Œè§’è‰²å½“å‰å¯é€‰å€¼ä¸º `system`ã€`user`ã€`assistant`å’Œ`tool`ã€‚ |
| streamï¼ˆå¯é€‰ï¼‰         | boolean    | False  | ç”¨äºæ§åˆ¶æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡ºã€‚                                   |
| do_rememberï¼ˆå¯é€‰ï¼‰    | boolean    | True   | æ¨¡å‹è®°å¿†èƒ½åŠ›å’Œå¤šè½®å¯¹è¯èƒ½åŠ›ã€‚                                 |
| toolsï¼ˆå¯é€‰ï¼‰          | list[dict] | None   | ç”¨äºæŒ‡å®šå¯ä¾›æ¨¡å‹è°ƒç”¨çš„å·¥å…·åº“ï¼Œä¸€æ¬¡å·¥å…·è°ƒç”¨æµç¨‹æ¨¡å‹ä¼šä»ä¸­é€‰æ‹©å…¶ä¸­ä¸€ä¸ªå·¥å…·ã€‚ |
| top_pï¼ˆå¯é€‰ï¼‰          | float      | 0.9    | ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ ¸é‡‡æ ·æ–¹æ³•æ¦‚ç‡é˜ˆå€¼ï¼Œä¾‹å¦‚ï¼Œå–å€¼ä¸º0.8æ—¶ï¼Œä»…ä¿ç•™æ¦‚ç‡åŠ èµ·æ¥å¤§äºç­‰äº0.8çš„æœ€å¯èƒ½tokençš„æœ€å°é›†åˆä½œä¸ºå€™é€‰é›†ã€‚ |
| top_kï¼ˆå¯é€‰ï¼‰          | integer    | 50     | ç”Ÿæˆæ—¶ï¼Œé‡‡æ ·å€™é€‰é›†çš„å¤§å°ã€‚ä¾‹å¦‚ï¼Œå–å€¼ä¸º50æ—¶ï¼Œä»…å°†å•æ¬¡ç”Ÿæˆä¸­å¾—åˆ†æœ€é«˜çš„50ä¸ª token ç»„æˆéšæœºé‡‡æ ·çš„å€™é€‰é›†ã€‚ |
| temperatureï¼ˆå¯é€‰ï¼‰    | float      | 0.7    | ç”¨äºæ§åˆ¶æ¨¡å‹å›å¤çš„éšæœºæ€§å’Œå¤šæ ·æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œtemperature å€¼æ§åˆ¶äº†ç”Ÿæˆæ–‡æœ¬æ—¶å¯¹æ¯ä¸ªå€™é€‰è¯çš„æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œå¹³æ»‘çš„ç¨‹åº¦ã€‚å–å€¼èŒƒå›´ï¼š[0, 1)ï¼Œä¸å»ºè®®å–å€¼ä¸º0ï¼Œæ— æ„ä¹‰ã€‚ |
| max_new_tokensï¼ˆå¯é€‰ï¼‰ | integer    | 512    | æ§åˆ¶æ¨¡å‹ç”Ÿæˆçš„æœ€å¤§ token æ•°ã€‚                                |

### è°ƒç”¨ç¤ºä¾‹

- **å¤šè½®å¯¹è¯ï¼š**

```python
messages = [
    {
        "role": "system",
        "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºè§£ç­”å„ç§é—®é¢˜çš„åŠ©æ‰‹ï¼Œä½ çš„ä»»åŠ¡æ˜¯ä¸ºç”¨æˆ·æä¾›ä¸“ä¸šã€å‡†ç¡®ã€æœ‰è§åœ°çš„å»ºè®®ã€‚"
    },
    {
        "role": "user",
        "content": "ä½ å¥½ï¼Œä½ æ˜¯è°ï¼Ÿ"
    }
]
resp = call_llm("qwen-2-7b-instruct", messages=messages)
print(resp)
```
æ¨¡å‹å›å¤æ ¼å¼å¦‚ä¸‹ï¼š
```json
{
  "status": 0,
  "data": {
    "role": "assistant",
    "content": "ä½ å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œä¸“é—¨ç”¨æ¥å¸®åŠ©è§£ç­”å„ç§é—®é¢˜ã€æä¾›ä¿¡æ¯å’Œè¿›è¡Œå¯¹è¯ã€‚æ— è®ºä½ éœ€è¦å­¦æœ¯æŒ‡å¯¼ã€æ—¥å¸¸å»ºè®®è¿˜æ˜¯å¨±ä¹å†…å®¹ï¼Œæˆ‘éƒ½åœ¨è¿™é‡Œä¸ºä½ æœåŠ¡ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ",
    "tool_calls": None
  }
}
```

- **å·¥å…·è°ƒç”¨ï¼š**

```python

```